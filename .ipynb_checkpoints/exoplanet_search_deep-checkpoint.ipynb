{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for the notebook so the results are reproducible\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "#wanted to take a look\n",
    "np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exoplanets = pd.read_csv(os.path.join('Resources', 'cumulative.csv'))\n",
    "pd.set_option('display.max_columns', None)\n",
    "exoplanets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows (if any) where koi_pdisposition is not FALSE POSITIVE or CANDIDATE; koi_disposition has additional categories\n",
    "exoplanets.koi_pdisposition.unique()\n",
    "#None found in current file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make koi_pdisposition and koi_disposition numerical variables, see if they are the same (no, koi_disposition has more categories)\n",
    "exoplanets_pdisp_cat = pd.get_dummies(exoplanets, prefix=['koi_pdisposition'], columns=['koi_pdisposition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exoplanets_disp_cat = pd.get_dummies(exoplanets_pdisp_cat, prefix=['koi_disposition'], columns=['koi_disposition'])\n",
    "exoplanets_disp_cat.drop('koi_pdisposition_FALSE POSITIVE', axis=1, inplace=True)\n",
    "\n",
    "exoplanets_disp_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop error columns (although these could be useful in the real world), extra IDs, KOI score, \n",
    "#and extra evaluation columns\n",
    "exoplanets_basic = exoplanets_disp_cat.drop([\"rowid\", \"kepoi_name\", \"kepler_name\", \"koi_score\", \n",
    "                                             \"koi_period_err1\", \"koi_period_err2\", \"koi_time0bk_err1\", \n",
    "                                             \"koi_time0bk_err2\", \"koi_impact_err1\", \"koi_impact_err2\", \n",
    "                                             \"koi_duration_err1\", \"koi_duration_err2\", \"koi_depth_err1\", \n",
    "                                             \"koi_depth_err2\", \"koi_prad_err1\", \"koi_prad_err2\", \"koi_teq_err1\", \n",
    "                                             \"koi_teq_err2\", \"koi_insol_err1\", \"koi_insol_err2\", \"koi_tce_plnt_num\", \n",
    "                                             \"koi_tce_delivname\",\"koi_steff_err1\", \"koi_steff_err2\", \n",
    "                                             \"koi_slogg_err1\", \"koi_slogg_err2\", \"koi_srad_err1\", \"koi_srad_err2\", \n",
    "                                             \"koi_disposition_CANDIDATE\", \"koi_disposition_CONFIRMED\", \n",
    "                                             \"koi_disposition_FALSE POSITIVE\"], axis=1)\n",
    "exoplanets_basic.rename(columns={'koi_fpflag_nt': 'flag_not_transit_like', \n",
    "                   'koi_fpflag_ss': 'flag_stellar_eclipse', \n",
    "                   'koi_fpflag_co': 'flag_centroid_offset',\n",
    "                   'koi_fpflag_ec': 'flag_ephemeris match',                  \n",
    "                   'koi_period': 'orbital_period',                  \n",
    "                   'koi_time0bk': 'time_first_trans_detected',\n",
    "                   'koi_impact': 'star_planet_dist_at_conj',                   \n",
    "                   'koi_duration': 'trans_duration',                   \n",
    "                   'koi_depth': 'stellar_flux_loss_at_trans_min',\n",
    "                   'koi_prad': 'planet_radius',\n",
    "                   'koi_teq': 'approx_planet_temp',\n",
    "                   'koi_insol': 'insolation_flux',\n",
    "                   'koi_model_snr': 'trans_sig_to_noise',\n",
    "                   'koi_steff': 'stellar_eff_temp',\n",
    "                   'koi_slogg': 'stellar_surf_gravity',\n",
    "                   'koi_srad': 'stellar_photosph_rad',\n",
    "                   'ra': 'sky_location_right_asc',\n",
    "                   'dec': 'sky_location_declination',                   \n",
    "                   'koi_kepmag': 'stellar_magnitude'}, inplace=True)\n",
    "exoplanets_basic.dropna(axis=0)\n",
    "#No na found by this method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(exoplanets_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isfinite(exoplanets_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "clean_dataset(exoplanets_basic)\n",
    "exoplanets_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(exoplanets_basic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isfinite(exoplanets_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = exoplanets_basic.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = exoplanets_basic[\"koi_pdisposition_CANDIDATE\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=20))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network 1 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run results\n",
    "\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 100)               2100      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 12,402\n",
    "Trainable params: 12,402\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 1s - loss: 0.0306 - accuracy: 0.9887\n",
    "Normal Neural Network 1 - Loss: 0.05442659353351464, Accuracy: 0.9886956810951233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressively accurate - but includes Kepler evaluations in \"flag\" columns. What happens if these are removed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second run: Same parameters, but remove \"flag\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deflag = exoplanets_basic.drop([\"kepid\", 'flag_not_transit_like','flag_centroid_offset','flag_stellar_eclipse',\n",
    "                                     'flag_ephemeris match' ], axis=1)\n",
    "feature_names = data_deflag.columns\n",
    "target = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "target_names = [\"False_positive\", \"Candidate\"]\n",
    "data_deflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_deflag.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=15))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network 2 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second run results\n",
    "\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_3 (Dense)              (None, 100)               1600      \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 11,902\n",
    "Trainable params: 11,902\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 0s - loss: 0.3951 - accuracy: 0.8122\n",
    "Normal Neural Network 2 - Loss: 0.43118020280547764, Accuracy: 0.8121739029884338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing \"flag\" columns reduces accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third run: \"Flag\" parameters removed, add one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_deflag.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=15))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(f\"Normal Neural Network 3 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third run results\n",
    "\n",
    "Model: \"sequential_2\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_6 (Dense)              (None, 100)               1600      \n",
    "_________________________________________________________________\n",
    "dense_7 (Dense)              (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_8 (Dense)              (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_9 (Dense)              (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 22,002\n",
    "Trainable params: 22,002\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 1s - loss: 0.4006 - accuracy: 0.8113\n",
    "Normal Neural Network 3 - Loss: 0.4283171708687492, Accuracy: 0.8113043308258057\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No noticeable gain in accuracy with added layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth run: same as third, but with different starting state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_deflag.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=18)\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=15))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(f\"Normal Neural Network 4 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth run results\n",
    "\n",
    "Model: \"sequential_3\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_10 (Dense)             (None, 100)               1600      \n",
    "_________________________________________________________________\n",
    "dense_11 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_12 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_13 (Dense)             (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 22,002\n",
    "Trainable params: 22,002\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 1s - loss: 0.4833 - accuracy: 0.8091\n",
    "Normal Neural Network 4 - Loss: 0.4340094618175341, Accuracy: 0.8091304302215576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy comparable with data split in third run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth run: same as third, but with twice the units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_deflag.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=200, activation='relu', input_dim=15))\n",
    "model.add(Dense(units=200, activation='relu'))\n",
    "model.add(Dense(units=200, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(f\"Normal Neural Network 5 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth run results\n",
    "\n",
    "Model: \"sequential_4\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_14 (Dense)             (None, 200)               3200      \n",
    "_________________________________________________________________\n",
    "dense_15 (Dense)             (None, 200)               40200     \n",
    "_________________________________________________________________\n",
    "dense_16 (Dense)             (None, 200)               40200     \n",
    "_________________________________________________________________\n",
    "dense_17 (Dense)             (None, 2)                 402       \n",
    "=================================================================\n",
    "Total params: 84,002\n",
    "Trainable params: 84,002\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 1s - loss: 0.3742 - accuracy: 0.8117\n",
    "Normal Neural Network 5 - Loss: 0.4426208177338476, Accuracy: 0.8117391467094421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No change in accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth run: same as third, but with twice the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_deflag.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=15))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=120,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(f\"Normal Neural Network 6 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sixth run results\n",
    "\n",
    "Model: \"sequential_5\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_18 (Dense)             (None, 100)               1600      \n",
    "_________________________________________________________________\n",
    "dense_19 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_20 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_21 (Dense)             (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 22,002\n",
    "Trainable params: 22,002\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 1s - loss: 0.3631 - accuracy: 0.8104\n",
    "Normal Neural Network 6 - Loss: 0.4818687497014585, Accuracy: 0.8104347586631775"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No noticeable gain in accuracy with added epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seventh run: same as third, but try \"Nadam\" normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (just messing, I really don't understand how these work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_deflag.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=15))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(f\"Normal Neural Network 7 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seventh run results\n",
    "\n",
    "Model: \"sequential_6\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_22 (Dense)             (None, 100)               1600      \n",
    "_________________________________________________________________\n",
    "dense_23 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_24 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_25 (Dense)             (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 22,002\n",
    "Trainable params: 22,002\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 1s - loss: 0.3636 - accuracy: 0.8187\n",
    "Normal Neural Network 7 - Loss: 0.42853991181954093, Accuracy: 0.8186956644058228\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eighth run: same as third, but try \"Adamax\" normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_deflag.drop(\"koi_pdisposition_CANDIDATE\", axis=1)\n",
    "y = data_deflag[\"koi_pdisposition_CANDIDATE\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=15))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(f\"Normal Neural Network 8 - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eighth run results\n",
    "\n",
    "Model: \"sequential_7\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_26 (Dense)             (None, 100)               1600      \n",
    "_________________________________________________________________\n",
    "dense_27 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_28 (Dense)             (None, 100)               10100     \n",
    "_________________________________________________________________\n",
    "dense_29 (Dense)             (None, 2)                 202       \n",
    "=================================================================\n",
    "Total params: 22,002\n",
    "Trainable params: 22,002\n",
    "Non-trainable params: 0\n",
    "    \n",
    "2300/1 - 1s - loss: 0.4071 - accuracy: 0.8122\n",
    "Normal Neural Network 8 - Loss: 0.43720548977022583, Accuracy: 0.8121739029884338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
